{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "servers = [\"Server_1\", \"Server_2\", \"Server_3\", \"Server_4\", \"Server_5\", \"Server_6\"]\n",
    "\n",
    "days = [\"Day_1\", \"Day_2\", \"Day_3\", \"Day_4\", \"Day_5\", \"Day_6\", \"Day_7\", \"Day_8\", \"Day_9\", \"Day_10\", \"Day_11\", \"Day_12\", \"Day_13\", \"Day_14\"]\n",
    "\n",
    "columns = [\"node_load1\",\n",
    "           \"node_memory_MemAvailable_bytes\",\n",
    "           \"node_network_transmit_bytes_total{device=\\\"eno1\\\"}\"]\n",
    "\n",
    "for server in servers:\n",
    "    for day in days:\n",
    "        if not os.path.exists(f\"{server}_Training_Sets/{day}.xlsx\"):\n",
    "            server_metrics_df = pd.read_csv(f\"Training_Sets/Training_Set_{day}/physical_level/{server.lower()}.csv\", usecols=columns, sep=\";\")\n",
    "\n",
    "            for column in columns:\n",
    "                server_metrics_df[column] = pd.to_numeric(server_metrics_df[column], errors='coerce')\n",
    "\n",
    "            server_metrics_df.to_excel(f\"{server}_Training_Sets/{day}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/Day_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(server_metrics_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "def random_sample_selection(time_series_df, num_samples):\n",
    "    \"\"\"\n",
    "    Seleciona aleatoriamente 'num_samples' amostras do DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        time_series_df (pd.DataFrame): DataFrame com features e target.\n",
    "        num_samples (int): Número de amostras desejadas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo as amostras selecionadas.\n",
    "    \"\"\"\n",
    "    # Remover linhas com NaNs para evitar problemas\n",
    "    #time_series_df = time_series_df.dropna()\n",
    "\n",
    "    # Garantir que o número de amostras não exceda o total disponível\n",
    "    num_samples_to_select = min(num_samples, len(time_series_df))\n",
    "\n",
    "    # Selecionar amostras aleatórias\n",
    "    selected_df = time_series_df.sample(n=num_samples_to_select, random_state=42)\n",
    "\n",
    "    print(f\"Amostras selecionadas aleatoriamente: {num_samples_to_select}\")\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/Day_14.xlsx\")[\"node_load_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento total do dataset\n",
    "dataset_length = len(server_metrics_df)\n",
    "\n",
    "# Pegar as 40 primeiras amostras para servir como features\n",
    "X = server_metrics_df[0:40]\n",
    "\n",
    "# Pegar a amostra na posição 40 + 400 para servir como label\n",
    "y = server_metrics_df[40 + 400]\n",
    "\n",
    "# Criar um dataframe com base nesta amostra\n",
    "time_series_df = pd.DataFrame(columns=[f\"X{idx+1}\" for idx in range(40)] + [\"y\"])\n",
    "\n",
    "time_series_df.loc[0] = list(X) + [y]\n",
    "time_series_df\n",
    "# Os valores de X1 a X40 estão corretos e y é tomado em relação à X40, ou seja, está 440 amostras a frente de X40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Carrega o server_metrics com a quantidade de amostras desejada\n",
    "server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/Day_14.xlsx\", nrows=10000)[\"node_load1\"]\n",
    "\n",
    "# Comprimento total do dataset\n",
    "dataset_length = len(server_metrics_df)\n",
    "\n",
    "# Criar um dataframe com base nesta amostra\n",
    "time_series_df = pd.DataFrame(columns=[f\"X{idx+1}\" for idx in range(40)] + [\"y\"])\n",
    "\n",
    "for idx in tqdm(range(dataset_length - (40 + 400))):\n",
    "    # Pegar as 40 primeiras amostras para servir como features\n",
    "    X = server_metrics_df[idx:(40 + idx)]\n",
    "\n",
    "    # Pegar a amostra na posição 40 + 400 para servir como label\n",
    "    y = server_metrics_df[(40 + idx) + 400 - 1] # O -1 é inserido devido ao index para ser compatível com o início no index 0\n",
    "\n",
    "    # Atribuindo o X e o y formados para o dataset\n",
    "    time_series_df.loc[idx] = list(X) + [y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No laço abaixo, o primeiro (40 + 400) está relacionado com o dataset original. Já o segundo é inserido para que o novo dataset time_series não sofra erro na comparação com X1.\n",
    "for idx in tqdm(range(0, dataset_length - (40 + 400) - (40 + 400))):\n",
    "    assert time_series_df[\"y\"][idx] == time_series_df[\"X1\"][(40 + idx - 1) + 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def time_series_formation(dataset, lookback, step):\n",
    "    # Comprimento total do dataset\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    # Criar um dataframe com base nesta amostra\n",
    "    time_series_df = pd.DataFrame(columns=[f\"X{idx+1}\" for idx in range(40)] + [\"y\"])\n",
    "\n",
    "    for idx in tqdm(range(dataset_length - (lookback + step))):\n",
    "        # Pegar as 40 primeiras amostras para servir como features\n",
    "        X = dataset[idx:(lookback + idx)]\n",
    "\n",
    "        # Pegar a amostra na posição 40 + 400 para servir como label\n",
    "        y = dataset[(lookback + idx) + step - 1] # O -1 é inserido devido ao index para ser compatível com o início no index 0\n",
    "\n",
    "        # Atribuindo o X e o y formados para o dataset\n",
    "        time_series_df.loc[idx] = list(X) + [y]\n",
    "\n",
    "    # No laço abaixo, o primeiro (40 + 400) está relacionado com o dataset original. Já o segundo é inserido para que o novo dataset time_series não sofra erro na comparação com X1.\n",
    "    for idx in tqdm(range(0, dataset_length - (lookback + step) - (lookback + step))):\n",
    "        assert time_series_df[\"y\"][idx] == time_series_df[\"X1\"][(lookback + idx - 1) + step]\n",
    "    \n",
    "    print(\"[TIME SERIES] O Dataset foi adequadamente formado!\")\n",
    "\n",
    "    return time_series_df\n",
    "\n",
    "def create_and_organize_datasets(pipeline, dataset_mode, dataset_folder):\n",
    "    os.makedirs(f\"Server_1_Training_Sets/{dataset_folder}\", exist_ok = True)\n",
    "\n",
    "    if dataset_mode == \"TRAIN\":\n",
    "        days = [\"Day_1\", \"Day_2\", \"Day_3\", \"Day_4\", \"Day_5\", \"Day_6\", \"Day_7\", \"Day_8\", \"Day_9\", \"Day_10\", \"Day_11\", \"Day_12\", \"Day_13\"]\n",
    "\n",
    "    elif dataset_mode == \"TEST\":\n",
    "        days = [\"Day_14\"]\n",
    "\n",
    "    global_representative_df = pd.DataFrame()\n",
    "\n",
    "    for day in days:\n",
    "        print(f\"\\nProcessando {day}...\")\n",
    "\n",
    "        #server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/{day}.xlsx\")\n",
    "        server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/{day}.xlsx\", nrows=10000)\n",
    "\n",
    "        step = 400  # Previsão 400 amostras à frente\n",
    "        lookback = 40\n",
    "\n",
    "        # CPU\n",
    "        if pipeline == \"CPU\":\n",
    "            dataset = np.array(server_metrics_df[\"node_load1\"])\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_14.xlsx\"\n",
    "\n",
    "        # RAM\n",
    "        if pipeline == \"RAM\":\n",
    "            dataset = np.array(server_metrics_df[\"node_memory_MemAvailable_bytes\"])\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_14.xlsx\"\n",
    "        \n",
    "        # Rede\n",
    "        # No caso do atributo de rede é necessário derivar.\n",
    "        if pipeline == \"NETWORK\":\n",
    "            dataset = np.array(server_metrics_df[\"node_network_transmit_bytes_total{device=\\\"eno1\\\"}\"])\n",
    "            dataset = pd.Series([dataset[idx + 1] - dataset[idx] for idx in range(len(dataset) - 1)])\n",
    "            dataset = dataset.drop_duplicates()\n",
    "            dataset = dataset[dataset < dataset.mean() + 4 * dataset.std()]\n",
    "            dataset = dataset.reset_index(drop=True)\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_14.xlsx\"\n",
    "\n",
    "        # Construir as features (X) e o target (y)\n",
    "        time_series_df = time_series_formation(dataset, lookback, step)\n",
    "\n",
    "        # Tratamento de NaNs\n",
    "        time_series_df = time_series_df.dropna()\n",
    "\n",
    "        # Chamar a função de seleção aleatória\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            #representative_df = random_sample_selection(time_series_df, 10000)\n",
    "            representative_df = time_series_df[:10000]\n",
    "\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            representative_df = time_series_df.iloc[:10000]\n",
    "\n",
    "        global_representative_df = pd.concat([global_representative_df, representative_df], ignore_index=True)\n",
    "\n",
    "    # Salvar dataset\n",
    "    global_representative_df.to_excel(file_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\"CPU\"]\n",
    "dataset_modes = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    for dataset_mode in dataset_modes:\n",
    "        create_and_organize_datasets(pipeline=pipeline, dataset_mode=dataset_mode, dataset_folder=\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def normalize_datasets(pipeline, dataset_mode, dataset_folder):\n",
    "    normalization_folder = f\"Server_1_Training_Sets/{dataset_folder}/normalization\"\n",
    "    os.makedirs(normalization_folder, exist_ok=True)\n",
    "\n",
    "    if pipeline == \"CPU\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/cpu_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/cpu_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_cpu.json\"\n",
    "\n",
    "    if pipeline == \"RAM\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/ram_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/ram_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_ram.json\"\n",
    "\n",
    "    if pipeline == \"NETWORK\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/network_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/network_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_network.json\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_dataset_path):\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Normalização iniciada\")\n",
    "\n",
    "        # Carregando o dataset\n",
    "        dataset = pd.read_excel(dataset_path)\n",
    "\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "\n",
    "            # Inicializar dicionários para armazenar os valores máximos de cada coluna dentro de 3 desvios padrões\n",
    "            max_values = {}\n",
    "\n",
    "            # Iterar sobre as colunas de features X1 até X40\n",
    "            for col in [f\"X{i}\" for i in range(1, 41)]:\n",
    "                # Calcular a média e o desvio padrão para a coluna atual\n",
    "                mean_col = dataset[col].mean()\n",
    "                std_col = dataset[col].std()\n",
    "                \n",
    "                # Definir os limites de 3 desvios padrões\n",
    "                lower_limit = mean_col - 3 * std_col\n",
    "                upper_limit = mean_col + 3 * std_col\n",
    "                \n",
    "                # Filtrar os dados dentro dos limites e encontrar o valor máximo\n",
    "                max_value = dataset[(dataset[col] >= lower_limit) & (dataset[col] <= upper_limit)][col].max()\n",
    "                max_values[col] = max_value\n",
    "\n",
    "            # Fazer o mesmo para a coluna 'y'\n",
    "            mean_y = dataset['y'].mean()\n",
    "            std_y = dataset['y'].std()\n",
    "            lower_limit_y = mean_y - 3 * std_y\n",
    "            upper_limit_y = mean_y + 3 * std_y\n",
    "            max_y = dataset[(dataset['y'] >= lower_limit_y) & (dataset['y'] <= upper_limit_y)]['y'].max()\n",
    "            max_values['y'] = max_y\n",
    "\n",
    "            max_values_converted = {key: int(value) if isinstance(value, np.integer) else float(value) for key, value in max_values.items()}\n",
    "\n",
    "            # Salvar o dicionário convertido em um arquivo JSON\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(max_values_converted, f, indent=4)\n",
    "\n",
    "        # Carregar o JSON como um dicionário\n",
    "        with open(json_path, \"r\") as f:\n",
    "            max_values = json.load(f)\n",
    "\n",
    "        # Normalizar o dataset usando os valores máximos encontrados\n",
    "        normalized_dataset = dataset.copy()\n",
    "        for col in max_values.keys():\n",
    "            normalized_dataset[col] = normalized_dataset[col] / max_values[col]\n",
    "\n",
    "        # CPU\n",
    "        if pipeline == \"CPU\":\n",
    "            #...\n",
    "            pass\n",
    "            \n",
    "        # RAM\n",
    "        if pipeline == \"RAM\":\n",
    "            normalized_dataset = 1 - normalized_dataset\n",
    "            normalized_dataset[normalized_dataset < 0] = 0\n",
    "\n",
    "        # Network\n",
    "        if pipeline == \"NETWORK\":\n",
    "            normalized_dataset[normalized_dataset < 0] = 0\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Normalização finalizada\")\n",
    "\n",
    "        normalized_dataset.to_excel(preprocessed_dataset_path, index=False)\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Conjunto de dados normalizado salvo\")\n",
    "\n",
    "    else:\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] O dataset já foi construído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\"CPU\"]\n",
    "dataset_modes = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    for dataset_mode in dataset_modes:\n",
    "        normalize_datasets(pipeline=pipeline, dataset_mode=dataset_mode, dataset_folder=\"datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mwml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
