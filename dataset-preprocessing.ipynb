{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reorganizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "servers = [\"Server_1\", \"Server_2\", \"Server_3\", \"Server_4\", \"Server_5\", \"Server_6\"]\n",
    "\n",
    "days = [\"Day_1\", \"Day_2\", \"Day_3\", \"Day_4\", \"Day_5\", \"Day_6\", \"Day_7\", \"Day_8\", \"Day_9\", \"Day_10\", \"Day_11\", \"Day_12\", \"Day_13\", \"Day_14\"]\n",
    "\n",
    "columns = [\"node_load1\",\n",
    "           \"node_memory_MemAvailable_bytes\",\n",
    "           \"node_network_transmit_bytes_total{device=\\\"eno1\\\"}\"]\n",
    "\n",
    "for server in servers:\n",
    "    for day in days:\n",
    "        if not os.path.exists(f\"{server}_Training_Sets/{day}.xlsx\"):\n",
    "            server_metrics_df = pd.read_csv(f\"Training_Sets/Training_Set_{day}/physical_level/{server.lower()}.csv\", usecols=columns, sep=\";\")\n",
    "\n",
    "            for column in columns:\n",
    "                server_metrics_df[column] = pd.to_numeric(server_metrics_df[column], errors='coerce')\n",
    "\n",
    "            server_metrics_df.to_excel(f\"{server}_Training_Sets/{day}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/Day_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(server_metrics_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def random_sample_selection(time_series_df, num_samples):\n",
    "    # Remover linhas com NaNs para evitar problemas\n",
    "    #time_series_df = time_series_df.dropna()\n",
    "\n",
    "    # Garantir que o número de amostras não exceda o total disponível\n",
    "    num_samples_to_select = min(num_samples, len(time_series_df))\n",
    "\n",
    "    # Selecionar amostras aleatórias\n",
    "    selected_df = time_series_df.sample(n=num_samples_to_select, random_state=42)\n",
    "\n",
    "    #print(f\"Amostras selecionadas aleatoriamente: {num_samples_to_select}\")\n",
    "    return selected_df\n",
    "\n",
    "def time_series_formation(dataset, lookback, step):\n",
    "    # Comprimento total do dataset\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    # Criar um dataframe com base nesta amostra\n",
    "    time_series_df = pd.DataFrame(columns=[f\"X{idx+1}\" for idx in range(lookback)] + [\"y\"])\n",
    "\n",
    "    for idx in tqdm(range(dataset_length - (lookback + step)), desc=\"Construindo séries\"):\n",
    "        # Pegar as 40 primeiras amostras para servir como features\n",
    "        X = dataset[idx:(lookback + idx)]\n",
    "\n",
    "        # Pegar a amostra na posição 40 + 400 para servir como label\n",
    "        y = dataset[(lookback + idx) + step - 1] # O -1 é inserido devido ao index para ser compatível com o início no index 0\n",
    "\n",
    "        # Atribuindo o X e o y formados para o dataset\n",
    "        time_series_df.loc[idx] = list(X) + [y]\n",
    "\n",
    "    # No laço abaixo, o primeiro (40 + 400) está relacionado com o dataset original. Já o segundo é inserido para que o novo dataset time_series não sofra erro na comparação com X1.\n",
    "    for idx in tqdm(range(0, dataset_length - (lookback + step) - (lookback + step))):\n",
    "        assert time_series_df[\"y\"][idx] == time_series_df[\"X1\"][(lookback + idx - 1) + step]\n",
    "    \n",
    "    print(\"[TIME SERIES] O Dataset foi adequadamente formado!\")\n",
    "\n",
    "    return time_series_df\n",
    "\n",
    "def create_and_organize_datasets(resampling_step, lookback, step, num_samples, pipeline, dataset_mode, dataset_folder):\n",
    "    os.makedirs(f\"Server_1_Training_Sets/{dataset_folder}\", exist_ok = True)\n",
    "\n",
    "    if dataset_mode == \"TRAIN\":\n",
    "        days = [\"Day_1\", \"Day_2\", \"Day_3\", \"Day_4\", \"Day_5\", \"Day_6\", \"Day_7\", \"Day_8\", \"Day_9\", \"Day_10\", \"Day_11\", \"Day_12\", \"Day_13\"]\n",
    "\n",
    "    elif dataset_mode == \"TEST\":\n",
    "        days = [\"Day_14\"]\n",
    "\n",
    "    global global_representative_df\n",
    "\n",
    "    global_representative_df = pd.DataFrame()\n",
    "\n",
    "    for day in days:\n",
    "        print(f\"\\nProcessando {day}...\")\n",
    "\n",
    "        #server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/{day}.xlsx\")\n",
    "        if num_samples:\n",
    "            server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/{day}.xlsx\", nrows=num_samples).reset_index(drop=True)\n",
    "        else:\n",
    "            server_metrics_df = pd.read_excel(f\"Server_1_Training_Sets/{day}.xlsx\").reset_index(drop=True)\n",
    "        \n",
    "        # Reamostragem\n",
    "\n",
    "        # Calcular os índices corretos: 0, 399, 799, 1199, ...\n",
    "        indices = [i * resampling_step - 1 for i in tqdm(range(1, (len(server_metrics_df) // resampling_step) + 1), desc=\"Reamostrado conjunto de dados\")]\n",
    "        indices.insert(0, 0)  # Garantir que o índice 0 esteja incluído\n",
    "\n",
    "        # Realizar a reamostragem\n",
    "        server_metrics_df = server_metrics_df.iloc[indices]\n",
    "\n",
    "        # CPU\n",
    "        if pipeline == \"CPU\":\n",
    "            dataset = np.array(server_metrics_df[\"node_load1\"])\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_14.xlsx\"\n",
    "\n",
    "        # RAM\n",
    "        if pipeline == \"RAM\":\n",
    "            dataset = np.array(server_metrics_df[\"node_memory_MemAvailable_bytes\"])\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_14.xlsx\"\n",
    "        \n",
    "        # Rede\n",
    "        # No caso do atributo de rede é necessário derivar.\n",
    "        if pipeline == \"NETWORK\":\n",
    "            dataset = np.array(server_metrics_df[\"node_network_transmit_bytes_total{device=\\\"eno1\\\"}\"])\n",
    "            dataset = pd.Series([dataset[idx + 1] - dataset[idx] for idx in range(len(dataset) - 1)])\n",
    "            dataset = dataset.drop_duplicates()\n",
    "            dataset = dataset[dataset < dataset.mean() + 4 * dataset.std()]\n",
    "            dataset = dataset.reset_index(drop=True)\n",
    "            if dataset_mode == \"TRAIN\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_1-13.xlsx\"\n",
    "            elif dataset_mode == \"TEST\":\n",
    "                file_save_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_14.xlsx\"\n",
    "\n",
    "        # Construir as features (X) e o target (y)\n",
    "        time_series_df = time_series_formation(dataset, lookback, step)\n",
    "\n",
    "        # Tratamento de NaNs\n",
    "        time_series_df = time_series_df.dropna()\n",
    "\n",
    "        # Chamar a função de seleção aleatória\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            #representative_df = random_sample_selection(time_series_df, 10000)\n",
    "            if num_samples:\n",
    "                representative_df = time_series_df.iloc[:num_samples]\n",
    "            else:\n",
    "                representative_df = time_series_df\n",
    "\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            if num_samples:\n",
    "                representative_df = time_series_df.iloc[:num_samples]\n",
    "            else:\n",
    "                representative_df = time_series_df\n",
    "\n",
    "        global_representative_df = pd.concat([global_representative_df, representative_df], ignore_index=True)\n",
    "\n",
    "    # Salvar dataset\n",
    "    global_representative_df.to_excel(file_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando Day_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2885713.22it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1616.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 106883.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2960685.18it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1847.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 108481.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3004874.51it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1801.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 106816.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2794908.73it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1841.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 108596.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2892623.45it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1882.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 108036.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2905145.63it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1875.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 103681.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2916367.82it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1874.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 107701.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2028820.21it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1676.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 102047.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3033549.85it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1858.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 106858.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2924841.53it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1868.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 101145.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2894009.47it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1838.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 107029.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2888473.34it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1807.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 109591.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2902353.56it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1815.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 99036.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3039656.65it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:00<00:00, 1846.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1419/1419 [00:00<00:00, 104275.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2735415.65it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1551.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 105385.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2930518.08it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1617.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 100236.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3048863.08it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1429/1429 [00:00<00:00, 1736.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1418/1418 [00:00<00:00, 104357.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3062777.77it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1426/1426 [00:00<00:00, 1728.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1415/1415 [00:00<00:00, 102400.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3022921.80it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1749.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 110678.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2959234.57it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1427/1427 [00:00<00:00, 1755.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1416/1416 [00:00<00:00, 102620.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2824975.57it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1751.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 109332.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2684354.56it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1523.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 105811.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2900959.54it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1704.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 106871.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2542002.42it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1655.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 108452.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 3036600.18it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1792.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 110047.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2578905.96it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1651.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 99186.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2826297.50it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1711.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 100490.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n",
      "\n",
      "Processando Day_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reamostrado conjunto de dados: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1440/1440 [00:00<00:00, 2858399.32it/s]\n",
      "Construindo séries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1428/1428 [00:00<00:00, 1710.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1417/1417 [00:00<00:00, 100973.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME SERIES] O Dataset foi adequadamente formado!\n"
     ]
    }
   ],
   "source": [
    "pipelines = [\"RAM\", \"NETWORK\"]\n",
    "dataset_modes = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "resampling_step = 400\n",
    "lookback = 10\n",
    "step = 1\n",
    "num_samples = None\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    for dataset_mode in dataset_modes:\n",
    "        create_and_organize_datasets(resampling_step=resampling_step,\n",
    "                                     lookback=lookback,\n",
    "                                     step=step,\n",
    "                                     num_samples=None,\n",
    "                                     pipeline=pipeline,\n",
    "                                     dataset_mode=dataset_mode,\n",
    "                                     dataset_folder=\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def normalize_datasets(lookback, pipeline, dataset_mode, dataset_folder):\n",
    "    normalization_folder = f\"Server_1_Training_Sets/{dataset_folder}/normalization\"\n",
    "    os.makedirs(normalization_folder, exist_ok=True)\n",
    "\n",
    "    if pipeline == \"CPU\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/cpu_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_load1_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/cpu_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_cpu.json\"\n",
    "\n",
    "    if pipeline == \"RAM\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/ram_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_memory_MemAvailable_bytes_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/ram_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_ram.json\"\n",
    "\n",
    "    if pipeline == \"NETWORK\":\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_1-13.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/network_usage_1-13.xlsx\"\n",
    "        elif dataset_mode == \"TEST\":\n",
    "            dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/node_network_transmit_bytes_total_14.xlsx\"\n",
    "            preprocessed_dataset_path = f\"Server_1_Training_Sets/{dataset_folder}/network_usage_14.xlsx\"\n",
    "        json_path = f\"Server_1_Training_Sets/{dataset_folder}/normalization/max_network.json\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_dataset_path):\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Normalização iniciada\")\n",
    "\n",
    "        # Carregando o dataset\n",
    "        dataset = pd.read_excel(dataset_path)\n",
    "\n",
    "        if dataset_mode == \"TRAIN\":\n",
    "\n",
    "            # Inicializar dicionários para armazenar os valores máximos de cada coluna dentro de 3 desvios padrões\n",
    "            max_values = {}\n",
    "\n",
    "            # Iterar sobre as colunas de features X1 até X40\n",
    "            for col in [f\"X{i}\" for i in range(1, lookback + 1)]:\n",
    "                # Calcular a média e o desvio padrão para a coluna atual\n",
    "                mean_col = dataset[col].mean()\n",
    "                std_col = dataset[col].std()\n",
    "                \n",
    "                # Definir os limites de 3 desvios padrões\n",
    "                lower_limit = mean_col - 3 * std_col\n",
    "                upper_limit = mean_col + 3 * std_col\n",
    "                \n",
    "                # Filtrar os dados dentro dos limites e encontrar o valor máximo\n",
    "                max_value = dataset[(dataset[col] >= lower_limit) & (dataset[col] <= upper_limit)][col].max()\n",
    "                max_values[col] = max_value\n",
    "\n",
    "            # Fazer o mesmo para a coluna 'y'\n",
    "            mean_y = dataset['y'].mean()\n",
    "            std_y = dataset['y'].std()\n",
    "            lower_limit_y = mean_y - 3 * std_y\n",
    "            upper_limit_y = mean_y + 3 * std_y\n",
    "            max_y = dataset[(dataset['y'] >= lower_limit_y) & (dataset['y'] <= upper_limit_y)]['y'].max()\n",
    "            max_values['y'] = max_y\n",
    "\n",
    "            max_values_converted = {key: int(value) if isinstance(value, np.integer) else float(value) for key, value in max_values.items()}\n",
    "\n",
    "            # Salvar o dicionário convertido em um arquivo JSON\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(max_values_converted, f, indent=4)\n",
    "\n",
    "        # Carregar o JSON como um dicionário\n",
    "        with open(json_path, \"r\") as f:\n",
    "            max_values = json.load(f)\n",
    "\n",
    "        # Normalizar o dataset usando os valores máximos encontrados\n",
    "        normalized_dataset = dataset.copy()\n",
    "        for col in max_values.keys():\n",
    "            normalized_dataset[col] = normalized_dataset[col] / max_values[col]\n",
    "\n",
    "        # CPU\n",
    "        if pipeline == \"CPU\":\n",
    "            #...\n",
    "            pass\n",
    "            \n",
    "        # RAM\n",
    "        if pipeline == \"RAM\":\n",
    "            normalized_dataset = 1 - normalized_dataset\n",
    "            normalized_dataset[normalized_dataset < 0] = 0\n",
    "\n",
    "        # Network\n",
    "        if pipeline == \"NETWORK\":\n",
    "            normalized_dataset[normalized_dataset < 0] = 0\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Normalização finalizada\")\n",
    "\n",
    "        normalized_dataset.to_excel(preprocessed_dataset_path, index=False)\n",
    "\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] Conjunto de dados normalizado salvo\")\n",
    "\n",
    "    else:\n",
    "        print(f\"[Pipeline: {pipeline} | Dataset Mode: {dataset_mode}] O dataset já foi construído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline: RAM | Dataset Mode: TRAIN] Normalização iniciada\n",
      "[Pipeline: RAM | Dataset Mode: TRAIN] Normalização finalizada\n",
      "[Pipeline: RAM | Dataset Mode: TRAIN] Conjunto de dados normalizado salvo\n",
      "[Pipeline: RAM | Dataset Mode: TEST] Normalização iniciada\n",
      "[Pipeline: RAM | Dataset Mode: TEST] Normalização finalizada\n",
      "[Pipeline: RAM | Dataset Mode: TEST] Conjunto de dados normalizado salvo\n",
      "[Pipeline: NETWORK | Dataset Mode: TRAIN] Normalização iniciada\n",
      "[Pipeline: NETWORK | Dataset Mode: TRAIN] Normalização finalizada\n",
      "[Pipeline: NETWORK | Dataset Mode: TRAIN] Conjunto de dados normalizado salvo\n",
      "[Pipeline: NETWORK | Dataset Mode: TEST] Normalização iniciada\n",
      "[Pipeline: NETWORK | Dataset Mode: TEST] Normalização finalizada\n",
      "[Pipeline: NETWORK | Dataset Mode: TEST] Conjunto de dados normalizado salvo\n"
     ]
    }
   ],
   "source": [
    "pipelines = [\"RAM\", \"NETWORK\"]\n",
    "dataset_modes = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    for dataset_mode in dataset_modes:\n",
    "        normalize_datasets(lookback=lookback, pipeline=pipeline, dataset_mode=dataset_mode, dataset_folder=\"datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
